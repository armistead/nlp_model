---
title: "milestone"
author: "armistead"
date: "January 8, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The goal of this paper is to demonstrate that I am on track to create my prediction alorithm for the Johns Hopkins University Data Science Capstone Project with SwiftKey. The project assignment is to build a text prediction algorithm based on the dataset similar to how a text message application recommends words as you type. It aims to solidify knowledge of tidying and modeling in R along with that of natural language processing.

## Getting set up

There are a few components to the dataset. First, these instructions assume you have already downloaded it and set your working directory to where you saved it. The link is here: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip. Because I am an English-speaker and expect my audience to be in the US, I will select the three text files starting with "en_US." Each files contains text from blog posts, news articles, and tweets: "blogs," "news," and "twitter" respectively. You will then need to load the necessary libraries and read the data into your R session.

```{r libs}
set.seed(37)

library(tidyverse) ## because we love it
library(stringi) ## for getting word counts
library(NLP)
library(SnowballC) ## for cleansing later on
library(RWeka)
library(tm) ## for building a corpus

blogs <- readLines(con <- file("./final/en_US/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

news <- readLines(con <- file("./final/en_US/en_US.news.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

twitter <- readLines(con <- file("./final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)

```

## Summary statistics

Next we need to get an idea of what size dataset we're working with. From the environment menu we can see that the files are 249, 19, and 301 MB respectively. Taking the word count and the linecount will give us a better sense of how to slice and dice it.

```{r pressure, echo=FALSE}
files <- Sys.glob("final/en_US/*.txt")

size_blogs <- file.info("./final/en_US/en_US.blogs.txt")$size / 1024^2 ## convert to MB
size_news <- file.info("./final/en_US/en_US.news.txt")$size / 1024^2
size_twitter <- file.info("./final/en_US/en_US.twitter.txt")$size / 1024^2
sizes <- rbind(size_blogs, size_news, size_twitter)

len_blogs <- length(blogs)
len_news <- length(news)
len_twitter <- length(twitter) #this may throw an error due to special characters. You can ignore.
lens <- rbind(len_blogs, len_news, len_twitter)

wordct_blogs <- sum(stri_count_words(blogs))
wordct_news <- sum(stri_count_words(news))
wordct_twitter <- sum(stri_count_words(twitter))
wordct <- rbind(wordct_blogs, wordct_news, wordct_twitter)

filestats <- data.frame(sizes, lens, wordct)
filestats
```

Now that we have a handle on the scope of the problem, we need to tidy up the data and take a sample.

```{r sample}
## take a random tenth of each text file
sample_blogs <- sample(blogs, size=len_blogs)
sample_news <- sample(news, size=len_news)
sample_twitter <- sample(twitter, size=len_twitter)#.2 inside parens to reduce size.

## the following sapply functions convert character vectors into whatever coding you provide. The i stands for internationalization though in this case we only want a latin ASCII framework.

sample_blogs <- sapply(sample_blogs,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_news <- sapply(sample_news,function(row) iconv(row, "utf-8", "ASCII", sub=""))
sample_twitter <- sapply(sample_twitter,function(row) iconv(row, "utf-8", "ASCII", sub=""))

sample_blogs <- (sample_blogs[!is.na(sample_blogs)])
sample_news <- (sample_blogs[!is.na(sample_blogs)])
sample_twitter <- (sample_blogs[!is.na(sample_blogs)])

sample <- sample(paste(sample_blogs, sample_news, sample_twitter), size=10000, replace = TRUE)

```

Here we create the corpus or body of natural language. A corpus can be a paragraph or simply a random collection of words or phrases. To get a better picture of our corpus however, we have to cleanse the data.

```{r corpus}

## Be careful in this section not to pass a character vector through the "tm" functions or else they will throw an error.

corpus <- VCorpus(VectorSource(sample))

# corpus <- lapply(corpus[1:2], as.character)

options(mc.cores = 1)

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removePunctuation))
corpus <- tm_map(corpus, content_transformer(stripWhitespace))    ## eliminate white space
corpus <- tm_map(corpus, content_transformer(removeNumbers))
corpus <- tm_map(corpus, content_transformer(stemDocument))   ## extracts stems of each word in vector
corpus <- tm_map(corpus, content_transformer(removeWords), stopwords(kind="SMART")) ## remove curse words
corpus <- tm_map(corpus, content_transformer(PlainTextDocument)) ## convert to txt doc because of corpora mappings

saveRDS(corpus, file = "./finalCorpus.RData") #for reference

```

## Tokenization and UniGram Frequency

Now that our data is cleansed and prepared for analysis, we have to performa a few steps. Tokenization is a kind of abstraction of natural language. It can be a word, phrase or chunk of words that contain meaning. Tokens are like strings in that they are character vectors, but differ in that they have a confined structure and meaning. For example below, we take the unigram, or one-word, words/phrases below and plot them in a histogram. That gives us a clear image of the most common tokens in this dataset.

```{r tokenization}
#Tokenize Data

data <- TermDocumentMatrix(corpus)

uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

data <- removeSparseTerms(data,.9998)

freq <- sort(colSums(as.matrix(data)), decreasing=TRUE)

triGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = triGramTokenizer)), .9998))
# use rm(list = ls()) and rerun if you get an error. May need to tweak ".9999" down.
biGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = biGramTokenizer)), .9998))
uniGramMatrix <- as.matrix(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = uniGramTokenizer)),.9998))

trifreqterm <- as.data.frame(rowSums(triGramMatrix))
bifreqterm <- as.data.frame(rowSums(biGramMatrix))
unifreqterm <- as.data.frame(rowSums(uniGramMatrix))

trifreqterm$trigram <- row.names(trifreqterm)
bifreqterm$bigram <- row.names(bifreqterm)
unifreqterm$unigram <- row.names(unifreqterm)

tritermtbl <- as.tibble(data.frame(trifreqterm[,2],trifreqterm[,1]))
bitermtbl <- as.tibble(data.frame(bifreqterm[,2],bifreqterm[,1]))
unitermtbl <- as.tibble(data.frame(unifreqterm[,2],unifreqterm[,1]))

names(unitermtbl) <- c("unigram","count")
names(bitermtbl) <- c("bigram","count")
names(tritermtbl) <- c("trigram","count")

uniplot <- subset(unitermtbl, count>2000)

x <- head(unitermtbl)
print(x)

uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
uniplotR <- uniplotR + geom_bar(stat = "identity") + 
        xlab("unigram") + 
        ylab("frequency") + 
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
        labs(title = "Unigram")
uniplotR

```

## BiGram Frequency
We do the same thing for two-word phrases.

```{r bigram}
#Plot BiGram Frequency
#bitermtbl <- arrange(bitermtbl, count)
biplot <- transform(bitermtbl,bigram=reorder(bigram, count))

x <- head(bitermtbl)
print(x)

#biplot <- subset(biplot, count>100)
# biplotorder <- reorder(biplot, count)

#uniplotR <- ggplot(uniplot, aes(reorder(unigram, -count), count))
#triplotR <- ggplot(subset(triplot, count>30), aes(reorder(trigram, -count), count))

biplotR <- ggplot(subset(biplot, count>100), aes(reorder(bigram, -count), count))
#aes(reorder(biplot, -count), count))
biplotR <- biplotR + geom_bar(stat = "identity") + 
        xlab("bigram") + 
        ylab("frequency") + 
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
        labs(title = "Bigram")
biplotR

```

## TriGram Frequency
Finally we set up and plot the three-word phrase element of the model.

```{r trigram}
#Plot TriGram Frequency
triplot <- transform(tritermtbl,trigram=reorder(trigram, -count))

x <- head(triplot)
print(x)

triplotR <- ggplot(subset(triplot, count>18), aes(reorder(trigram, -count), count))
#aes(reorder(biplot, -count), count))
triplotR <- triplotR + geom_bar(stat = "identity") + 
        xlab("trigram") + 
        ylab("frequency") + 
        theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
        labs(title = "Trigram")
triplotR

```

## Sidebar

In order to make the word look-up function easier in our Shiny App, we need to break up the n-gram lists into their own separate columns. It is not so much exploratory. However, it is essential for ease of use later.
```{r separate}

bigramX <- bitermtbl %>%
        separate(col = bigram, c("bi_word_1", "bi_word_2"), sep=" ") %>%
        arrange(desc(count)) %>%
        as.data.frame()

trigramX <- tritermtbl %>%
        separate(col = trigram, c("tri_word_1", "tri_word_2", "tri_word_3"), sep=" ") %>%
        arrange(desc(count)) %>%
        as.data.frame()

saveRDS(bigramX, "./nlp_model/bigram.RData")
saveRDS(trigramX, "./nlp_model/trigram.RData")

bigramR <- readRDS("./nlp_model/bigram.RData")
trigramR <- readRDS("./nlp_model/trigram.RData")


```


## Going forward

Going forward, my aim is to build on this analysis with a predictive model and Shiny app that recommends next-words. Uni, bi, and tri-gram words and phrases will be ranked by frequency. Other variables such as grammatical factors may also be incorporated into the predictive model. For the Shiny app, I intend to make it as simple as possible. I hope to give it the look and feel of Google.com's plain text box. Either way, the goal is to make the recommendations as natural as possible.
